{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using dynamic RNN and LSTM with TFLite converter\n",
    "\n",
    "This test script highlights several tricks you need in order to use Dynamic RNN and LSTM in TFLite.\n",
    "Hopefully this can save your time figuring out these experimental features!\n",
    "\n",
    "The major part of this script takes the reference from: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/examples/lstm/unidirectional_sequence_lstm_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.ops import control_flow_util\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "from tensorflow.lite.python.op_hint import convert_op_hints_to_stubs\n",
    "\n",
    "# Turn warning off\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0-dev20190227\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up mnist and initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# download and process mnist\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "# Define constants\n",
    "# Unrolled through 28 time steps\n",
    "time_steps = 28\n",
    "# Rows of 28 pixels\n",
    "n_input = 28\n",
    "# Learning rate for Adam optimizer\n",
    "learning_rate = 0.001\n",
    "# MNIST is meant to be classified in 10 classes(0-9).\n",
    "n_classes = 10\n",
    "# Batch size\n",
    "batch_size = 16\n",
    "# Lstm Units.\n",
    "num_units = 16\n",
    "TRAIN_STEPS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for building the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trick 1: \n",
    "Use `tf.lite.experimental.nn.TFLiteLSTMCell` for LSTM and `tf.lite.experimental.nn.dynamic_rnn` for dynamic rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildLstmLayer():\n",
    "    return tf.nn.rnn_cell.MultiRNNCell([\n",
    "        tf.lite.experimental.nn.TFLiteLSTMCell(\n",
    "            num_units, use_peepholes=True, forget_bias=0, name=\"rnn1\"),\n",
    "        tf.lite.experimental.nn.TFLiteLSTMCell(\n",
    "            num_units, num_proj=8, forget_bias=0, name=\"rnn2\"),\n",
    "        tf.lite.experimental.nn.TFLiteLSTMCell(\n",
    "            num_units // 2,\n",
    "            use_peepholes=True,\n",
    "            num_proj=8,\n",
    "            forget_bias=0,\n",
    "            name=\"rnn3\"),\n",
    "        tf.lite.experimental.nn.TFLiteLSTMCell(\n",
    "            num_units, forget_bias=0, name=\"rnn4\")\n",
    "    ])\n",
    "\n",
    "def buildModel(lstm_layer, is_dynamic_rnn):\n",
    "    # Weights and biases for output softmax layer.\n",
    "    out_weights = tf.Variable(\n",
    "        tf.random_normal([num_units, n_classes]))\n",
    "    out_bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "    # input image placeholder\n",
    "    x = tf.placeholder(\n",
    "        \"float\", [None, time_steps, n_input], name=\"INPUT_IMAGE\")\n",
    "\n",
    "    # x is shaped [batch_size,time_steps,num_inputs]\n",
    "    if is_dynamic_rnn:\n",
    "        lstm_input = tf.transpose(x, perm=[1, 0, 2])\n",
    "        outputs, _ = tf.lite.experimental.nn.dynamic_rnn(\n",
    "          lstm_layer, lstm_input, dtype=\"float32\")\n",
    "        outputs = tf.unstack(outputs, axis=0)\n",
    "    else:\n",
    "        lstm_input = tf.unstack(x, time_steps, 1)\n",
    "        outputs, _ = tf.nn.static_rnn(lstm_layer, lstm_input, dtype=\"float32\")\n",
    "\n",
    "    # Compute logits by multiplying outputs[-1] of shape [batch_size,num_units]\n",
    "    # by the softmax layer's out_weight of shape [num_units,n_classes]\n",
    "    # plus out_bias\n",
    "    prediction = tf.matmul(outputs[-1], out_weights) + out_bias\n",
    "    output_class = tf.nn.softmax(prediction, name=\"OUTPUT_CLASS\")\n",
    "\n",
    "    return x, prediction, output_class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions for training, saving/restoring, and serving (inferencing) the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(x, prediction, output_class, sess):\n",
    "    # input label placeholder\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "    # Loss function\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))\n",
    "    # Optimization\n",
    "    opt = tf.train.AdamOptimizer(\n",
    "        learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    # Initialize variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for _ in range(TRAIN_STEPS):\n",
    "        batch_x, batch_y = mnist.train.next_batch(\n",
    "          batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        batch_x = batch_x.reshape((batch_size, time_steps,\n",
    "                                 n_input))\n",
    "        sess.run(opt, feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "def saveAndRestoreModel(lstm_layer, sess, saver, is_dynamic_rnn):\n",
    "    model_dir = 'export/dynamic_rnn'\n",
    "    saver.save(sess, model_dir)\n",
    "\n",
    "    # Reset the graph.\n",
    "    tf.reset_default_graph()\n",
    "    x, prediction, output_class = buildModel(lstm_layer, is_dynamic_rnn)\n",
    "\n",
    "    new_sess = tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(new_sess, model_dir)\n",
    "    return x, prediction, output_class, new_sess\n",
    "\n",
    "def getInferenceResult(x, output_class, sess):\n",
    "    b1, _ = mnist.train.next_batch(batch_size=1)\n",
    "    sample_input = np.reshape(b1, (1, time_steps, n_input))\n",
    "    expected_output = sess.run(output_class, feed_dict={x: sample_input})\n",
    "    frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "        sess, sess.graph_def, [output_class.op.name])\n",
    "    with open('output_graph.pb', 'wb') as f:\n",
    "        f.write(frozen_graph.SerializeToString())\n",
    "    return sample_input, expected_output, frozen_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and run session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trick 2:\n",
    "Set `control_flow_util.ENABLE_CONTROL_FLOW_V2 = True` in order to use the `tf.lite.experimental.nn` implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_flow_util.ENABLE_CONTROL_FLOW_V2 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "x, prediction, output_class = buildModel(\n",
    "    buildLstmLayer(), is_dynamic_rnn=True)\n",
    "trainModel(x, prediction, output_class, sess)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "x, prediction, output_class, new_sess = saveAndRestoreModel(\n",
    "    buildLstmLayer(), sess, saver, is_dynamic_rnn=True)\n",
    "\n",
    "test_inputs, expected_output, frozen_graph = getInferenceResult(\n",
    "    x, output_class, new_sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for converting graph to TFLite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trick 3:\n",
    "Must use `convert_op_hints_to_stubs` to wrap a subpart of a TensorFlow execution graph to a single TensorFlow Lite operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfliteInvoke(graph, test_inputs, outputs, op_hints_to_stubs=True):\n",
    "    tf.reset_default_graph()\n",
    "    # Turn the input into placeholder of shape 1\n",
    "    tflite_input = tf.placeholder(\n",
    "        \"float\", [1, time_steps, n_input], name=\"INPUT_IMAGE_LITE\")\n",
    "    tf.import_graph_def(graph, name=\"\", input_map={\"INPUT_IMAGE\": tflite_input})\n",
    "    with tf.Session() as sess:\n",
    "        curr = sess.graph_def\n",
    "        if op_hints_to_stubs is True:\n",
    "            curr = convert_op_hints_to_stubs(graph_def=curr)\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter(curr, [tflite_input], [outputs])\n",
    "    tflite = converter.convert()\n",
    "    interpreter = tf.lite.Interpreter(model_content=tflite)\n",
    "\n",
    "    try:\n",
    "        interpreter.allocate_tensors()\n",
    "    except ValueError:\n",
    "        print('false')\n",
    "        assert False\n",
    "\n",
    "    input_index = (interpreter.get_input_details()[0][\"index\"])\n",
    "    interpreter.set_tensor(input_index, test_inputs)\n",
    "    interpreter.invoke()\n",
    "    output_index = (interpreter.get_output_details()[0][\"index\"])\n",
    "    result = interpreter.get_tensor(output_index)\n",
    "    # Reset all variables so it will not pollute other inferences.\n",
    "    interpreter.reset_all_variables()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to TFLite and compare result with inference with original graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "result = tfliteInvoke(frozen_graph, test_inputs, output_class)\n",
    "print(np.allclose(expected_output, result, rtol=1e-6, atol=1e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get some unsupported operation error if we did not appply `convert_op_hints_to_stubs` to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConverterError",
     "evalue": "TOCO failed. See console for info.\n2019-03-06 18:18:19.683408: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2019-03-06 18:18:19.706735: I tensorflow/lite/toco/import_tensorflow.cc:1335] Converting unsupported operation: TensorListFromTensor\n2019-03-06 18:18:19.706780: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\n2019-03-06 18:18:19.706826: I tensorflow/lite/toco/import_tensorflow.cc:1335] Converting unsupported operation: TensorListReserve\n2019-03-06 18:18:19.706855: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\n2019-03-06 18:18:19.706953: I tensorflow/lite/toco/import_tensorflow.cc:1335] Converting unsupported operation: While\n2019-03-06 18:18:19.707012: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\n2019-03-06 18:18:19.707018: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\n2019-03-06 18:18:19.707079: I tensorflow/lite/toco/import_tensorflow.cc:1335] Converting unsupported operation: TensorListStack\n2019-03-06 18:18:19.708798: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 19 operators, 185 arrays (0 quantized)\n2019-03-06 18:18:19.709206: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 19 operators, 185 arrays (0 quantized)\n2019-03-06 18:18:19.709661: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 8 operators, 171 arrays (0 quantized)\n2019-03-06 18:18:19.710038: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 8 operators, 171 arrays (0 quantized)\n2019-03-06 18:18:19.710310: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 8 operators, 171 arrays (0 quantized)\n2019-03-06 18:18:19.711074: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 3136 bytes, theoretical optimal value: 3136 bytes.\n2019-03-06 18:18:19.711646: E tensorflow/lite/toco/toco_tooling.cc:456] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\n and pasting the following:\n\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED, RESHAPE, SOFTMAX, UNPACK. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.\nTraceback (most recent call last):\n  File \"/anaconda2/envs/Bose3_night/bin/toco_from_protos\", line 11, in <module>\n    sys.exit(main())\n  File \"/anaconda2/envs/Bose3_night/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59, in main\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\n  File \"/anaconda2/envs/Bose3_night/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n  File \"/anaconda2/envs/Bose3_night/lib/python3.6/site-packages/absl/app.py\", line 300, in run\n    _run_main(main, args)\n  File \"/anaconda2/envs/Bose3_night/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\n    sys.exit(main(argv))\n  File \"/anaconda2/envs/Bose3_night/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33, in execute\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\n and pasting the following:\n\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED, RESHAPE, SOFTMAX, UNPACK. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConverterError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-381c75b1aec2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfliteInvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrozen_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_hints_to_stubs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-aed1c5812a7f>\u001b[0m in \u001b[0;36mtfliteInvoke\u001b[0;34m(graph, test_inputs, outputs, op_hints_to_stubs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtflite_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtflite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0minterpreter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterpreter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtflite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/Bose3_night/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    756\u001b[0m           \u001b[0minput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m           \u001b[0moutput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m           **converter_kwargs)\n\u001b[0m\u001b[1;32m    759\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m       result = _toco_convert_graph_def(\n",
      "\u001b[0;32m/anaconda2/envs/Bose3_night/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36mtoco_convert_impl\u001b[0;34m(input_data, input_tensors, output_tensors, *args, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m   data = toco_convert_protos(model_flags.SerializeToString(),\n\u001b[1;32m    441\u001b[0m                              \u001b[0mtoco_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m                              input_data.SerializeToString())\n\u001b[0m\u001b[1;32m    443\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/Bose3_night/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36mtoco_convert_protos\u001b[0;34m(model_flags_str, toco_flags_str, input_data_str)\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_convert_to_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m       raise ConverterError(\n\u001b[0;32m--> 205\u001b[0;31m           \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\n\u001b[0m\u001b[1;32m    206\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;31m# Must manually cleanup files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConverterError\u001b[0m: TOCO failed. See console for info.\n2019-03-06 18:18:19.683408: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2019-03-06 18:18:19.706735: I tensorflow/lite/toco/import_tensorflow.cc:1335] Converting unsupported operation: TensorListFromTensor\n2019-03-06 18:18:19.706780: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\n2019-03-06 18:18:19.706826: I tensorflow/lite/toco/import_tensorflow.cc:1335] Converting unsupported operation: TensorListReserve\n2019-03-06 18:18:19.706855: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\n2019-03-06 18:18:19.706953: I tensorflow/lite/toco/import_tensorflow.cc:1335] Converting unsupported operation: While\n2019-03-06 18:18:19.707012: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\n2019-03-06 18:18:19.707018: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\n2019-03-06 18:18:19.707079: I tensorflow/lite/toco/import_tensorflow.cc:1335] Converting unsupported operation: TensorListStack\n2019-03-06 18:18:19.708798: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 19 operators, 185 arrays (0 quantized)\n2019-03-06 18:18:19.709206: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 19 operators, 185 arrays (0 quantized)\n2019-03-06 18:18:19.709661: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 8 operators, 171 arrays (0 quantized)\n2019-03-06 18:18:19.710038: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 8 operators, 171 arrays (0 quantized)\n2019-03-06 18:18:19.710310: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 8 operators, 171 arrays (0 quantized)\n2019-03-06 18:18:19.711074: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 3136 bytes, theoretical optimal value: 3136 bytes.\n2019-03-06 18:18:19.711646: E tensorflow/lite/toco/toco_tooling.cc:456] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\n and pasting the following:\n\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED, RESHAPE, SOFTMAX, UNPACK. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.\nTraceback (most recent call last):\n  File \"/anaconda2/envs/Bose3_night/bin/toco_from_protos\", line 11, in <module>\n    sys.exit(main())\n  File \"/anaconda2/envs/Bose3_night/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59, in main\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\n  File \"/anaconda2/envs/Bose3_night/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n  File \"/anaconda2/envs/Bose3_night/lib/python3.6/site-packages/absl/app.py\", line 300, in run\n    _run_main(main, args)\n  File \"/anaconda2/envs/Bose3_night/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\n    sys.exit(main(argv))\n  File \"/anaconda2/envs/Bose3_night/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33, in execute\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\n and pasting the following:\n\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED, RESHAPE, SOFTMAX, UNPACK. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.\n\n\n"
     ]
    }
   ],
   "source": [
    "result = tfliteInvoke(frozen_graph, test_inputs, output_class, op_hints_to_stubs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
