{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using dynamic RNN and LSTM with TFLite converter\n",
    "\n",
    "This test script highlights the issue of unable to access the state while using TFLite interpreter.\n",
    "\n",
    "The major part of this script takes the reference from: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/examples/lstm/unidirectional_sequence_lstm_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/Bose_night/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.ops import control_flow_util\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "from tensorflow.lite.python.op_hint import convert_op_hints_to_stubs\n",
    "\n",
    "# Turn warning off\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0-dev20190228\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up mnist and initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# download and process mnist\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "# Define constants\n",
    "# Unrolled through 28 time steps\n",
    "time_steps = 28\n",
    "# Rows of 28 pixels\n",
    "n_input = 28\n",
    "# Learning rate for Adam optimizer\n",
    "learning_rate = 0.001\n",
    "# MNIST is meant to be classified in 10 classes(0-9).\n",
    "n_classes = 10\n",
    "# Batch size\n",
    "batch_size = 16\n",
    "# Lstm Units.\n",
    "num_units = 16\n",
    "TRAIN_STEPS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for building the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `tf.lite.experimental.nn.TFLiteLSTMCell` for LSTM and `tf.lite.experimental.nn.dynamic_rnn` for dynamic rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildLstmLayer():\n",
    "    return tf.nn.rnn_cell.MultiRNNCell([\n",
    "        tf.lite.experimental.nn.TFLiteLSTMCell(\n",
    "            num_units, use_peepholes=True, forget_bias=0, name=\"rnn1\"),\n",
    "        tf.lite.experimental.nn.TFLiteLSTMCell(\n",
    "            num_units, num_proj=8, forget_bias=0, name=\"rnn2\"),\n",
    "        tf.lite.experimental.nn.TFLiteLSTMCell(\n",
    "            num_units // 2,\n",
    "            use_peepholes=True,\n",
    "            num_proj=8,\n",
    "            forget_bias=0,\n",
    "            name=\"rnn3\"),\n",
    "        tf.lite.experimental.nn.TFLiteLSTMCell(\n",
    "            num_units, forget_bias=0, name=\"rnn4\")\n",
    "    ])\n",
    "\n",
    "def buildModel(lstm_layer, is_dynamic_rnn):\n",
    "    # Weights and biases for output softmax layer.\n",
    "    out_weights = tf.Variable(\n",
    "        tf.random_normal([num_units, n_classes]))\n",
    "    out_bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "    # input image placeholder\n",
    "    x = tf.placeholder(\n",
    "        \"float\", [None, time_steps, n_input], name=\"INPUT_IMAGE\")\n",
    "\n",
    "    # x is shaped [batch_size,time_steps,num_inputs]\n",
    "    if is_dynamic_rnn:\n",
    "        lstm_input = tf.transpose(x, perm=[1, 0, 2])\n",
    "        outputs, state = tf.lite.experimental.nn.dynamic_rnn(\n",
    "          lstm_layer, lstm_input, dtype=\"float32\", time_major=True)\n",
    "        outputs = tf.unstack(outputs, axis=0)\n",
    "    else:\n",
    "        lstm_input = tf.unstack(x, time_steps, 1)\n",
    "        outputs, state = tf.nn.static_rnn(lstm_layer, lstm_input, dtype=\"float32\")\n",
    "\n",
    "    # Compute logits by multiplying outputs[-1] of shape [batch_size,num_units]\n",
    "    # by the softmax layer's out_weight of shape [num_units,n_classes]\n",
    "    # plus out_bias\n",
    "    prediction = tf.matmul(outputs[-1], out_weights) + out_bias\n",
    "    output_class = tf.nn.softmax(prediction, name=\"OUTPUT_CLASS\")\n",
    "\n",
    "    state_c, state_h = state[0]\n",
    "    state_c = tf.expand_dims(state_c, 0)\n",
    "    state_h = tf.expand_dims(state_h, 0)\n",
    "\n",
    "    state_out = tf.identity(tf.concat([state_c, state_h], 0), name='state_out')\n",
    "\n",
    "    return x, prediction, output_class, state_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions for training, saving/restoring, and serving (inferencing) the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(x, prediction, output_class, sess):\n",
    "    # input label placeholder\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "    # Loss function\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))\n",
    "    # Optimization\n",
    "    opt = tf.train.AdamOptimizer(\n",
    "        learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    # Initialize variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for _ in range(TRAIN_STEPS):\n",
    "        batch_x, batch_y = mnist.train.next_batch(\n",
    "          batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        batch_x = batch_x.reshape((batch_size, time_steps,\n",
    "                                 n_input))\n",
    "        sess.run(opt, feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "def saveAndRestoreModel(lstm_layer, sess, saver, is_dynamic_rnn):\n",
    "    model_dir = 'export/dynamic_rnn'\n",
    "    saver.save(sess, model_dir)\n",
    "\n",
    "    # Reset the graph.\n",
    "    tf.reset_default_graph()\n",
    "    x, prediction, output_class, output_state = buildModel(lstm_layer, is_dynamic_rnn)\n",
    "\n",
    "    new_sess = tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(new_sess, model_dir)\n",
    "    return x, prediction, output_class, output_state, new_sess\n",
    "\n",
    "def getInferenceResult(x, output_class, output_state, sess):\n",
    "    b1, _ = mnist.train.next_batch(batch_size=1)\n",
    "    sample_input = np.reshape(b1, (1, time_steps, n_input))\n",
    "    [expected_output, expected_state] = sess.run([output_class, output_state], feed_dict={x: sample_input})\n",
    "    frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "        sess, sess.graph_def, [output_class.op.name, output_state.op.name])\n",
    "    with open('output_graph.pb', 'wb') as f:\n",
    "        f.write(frozen_graph.SerializeToString())\n",
    "    return sample_input, expected_output, expected_state, frozen_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and run session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_flow_util.ENABLE_CONTROL_FLOW_V2 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "x, prediction, output_class, output_state = buildModel(\n",
    "    buildLstmLayer(), is_dynamic_rnn=True)\n",
    "trainModel(x, prediction, output_class, sess)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "x, prediction, output_class, output_state, new_sess = saveAndRestoreModel(\n",
    "    buildLstmLayer(), sess, saver, is_dynamic_rnn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get inference result for sanity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs, expected_output, expected_state, frozen_graph = getInferenceResult(\n",
    "    x, output_class, output_state, new_sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for converting graph to TFLite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert frozen graph to tf lite model, perform inference with interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfliteInvoke(graph, test_inputs, outputs, output_state, state_out=False):\n",
    "    tf.reset_default_graph()\n",
    "    # Turn the input into placeholder of shape 1\n",
    "    tflite_input = tf.placeholder(\n",
    "        \"float\", [1, time_steps, n_input], name=\"INPUT_IMAGE_LITE\")\n",
    "    tf.import_graph_def(graph, name=\"\", input_map={\"INPUT_IMAGE\": tflite_input})\n",
    "    with tf.Session() as sess:\n",
    "        curr = sess.graph_def\n",
    "        curr = convert_op_hints_to_stubs(graph_def=curr)\n",
    "    \n",
    "    # if set, include state as output tensor\n",
    "    if state_out:\n",
    "        converter = tf.lite.TFLiteConverter(curr, [tflite_input], [outputs, output_state])\n",
    "    else:\n",
    "        converter = tf.lite.TFLiteConverter(curr, [tflite_input], [outputs])\n",
    "\n",
    "    tflite = converter.convert()\n",
    "    interpreter = tf.lite.Interpreter(model_content=tflite)\n",
    "\n",
    "    try:\n",
    "        interpreter.allocate_tensors()\n",
    "    except ValueError:\n",
    "        assert False\n",
    "\n",
    "    input_index = (interpreter.get_input_details()[0][\"index\"])\n",
    "    interpreter.set_tensor(input_index, test_inputs)\n",
    "    \n",
    "    # note: kernel will crash here if trying to include lstm state\n",
    "    interpreter.invoke()\n",
    "    output_index = (interpreter.get_output_details()[0][\"index\"])\n",
    "    result = interpreter.get_tensor(output_index)\n",
    "    \n",
    "    # if set, get state from interpreter\n",
    "    if state_out:\n",
    "        state_index = (interpreter.get_output_details()[1][\"index\"])\n",
    "        state = interpreter.get_tensor(state_index)\n",
    "    else:\n",
    "        state = None\n",
    "\n",
    "    # Reset all variables so it will not pollute other inferences.\n",
    "    interpreter.reset_all_variables()\n",
    "    \n",
    "    return result, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, state = tfliteInvoke(frozen_graph, test_inputs, output_class, output_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(expected_output, result, rtol=1e-6, atol=1e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel crashes while trying to access LSTM state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### note: kernel will crash in  `interpreter.invoke()` if trying to include lstm state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, state = tfliteInvoke(frozen_graph, test_inputs, output_class, output_state, state_out=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
